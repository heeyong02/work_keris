import re
import unicodedata
from io import BytesIO
from datetime import datetime

import pandas as pd
import streamlit as st
from loguru import logger

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="NCE ì „ê³µ ë¶„ë¥˜ ì¶”ì²œ ì‹œìŠ¤í…œ", page_icon="ğŸ«", layout="wide"
)

# ì œëª©
st.title("ğŸ« NCE ì „ê³µ í‘œì¤€ë¶„ë¥˜ì²´ê³„ ì¶”ì²œ ì‹œìŠ¤í…œ")
st.markdown("---")

# ì‚¬ì´ë“œë°”
st.sidebar.header("ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ")


# === í•¨ìˆ˜ ì •ì˜ ===
# ì „ê°â†’ë°˜ê° ì •ê·œí™”
def nfkc(x):
    return unicodedata.normalize("NFKC", str(x)) if pd.notna(x) else x


# engâ†’kor ì‚¬ì „ ìƒì„±
def build_eng2kor(series):
    eng2kor = {}
    norm = series.dropna().map(nfkc).unique().tolist()
    for c in norm:
        c_std = c.replace("(", "(").replace(")", ")")
        kor = re.sub(r"\(.*?\)", "", c_std).strip()
        inside = re.findall(r"\((.*?)\)", c_std)
        if inside:
            inside_text = inside[0].strip()
            if re.search(r"[A-Za-z]", inside_text):
                eng = re.sub(r"\d+([\-\.]\d+)?$", "", inside_text).strip()
                eng2kor[eng.lower()] = kor
    return eng2kor


# ê³¼ëª© ì •ì œ í•¨ìˆ˜
def clean_course(name, eng2kor=None):
    if pd.isna(name):
        return None

    raw = nfkc(name).strip()

    if eng2kor:
        raw_lower = raw.lower()
        for eng, kor in eng2kor.items():
            if eng and eng in raw_lower:
                return kor

    txt = raw
    txt = re.sub(r"\[.*?\]", "", txt)

    while re.search(r"\([^()]*\)", txt):
        txt = re.sub(r"\([^()]*\)", "", txt)
        txt = re.sub(r"(?<=[ê°€-í£A-Za-z])[\-\.]?\d+$", "", txt)

    txt = re.sub(r"[ï½@#â˜†â… â…¡â…¢â…£â…¤â…¥â…¦â…§â…¨â…©]", "", txt)
    txt = re.sub(r"(?<=[ê°€-í£])(I|II|III|IV|V|VI|VII|VIII|IX|X)$", "", txt)
    txt = re.sub(r"(?<=[ê°€-í£A-Za-z])\d+(?=[ê°€-í£A-Za-z])", "", txt)
    txt = re.sub(r"(?<=[ê°€-í£A-Za-z])\d+$", "", txt).strip()
    txt = re.sub(r"\s+", " ", txt).strip()
    txt = re.sub(r"\s", "", txt).strip()

    if txt in ["", "/", "-", "--", "NULL", "NaN", "ì—†ìŒ"]:
        return None
    return txt


# ìˆ«ì ë²„ì „ ê³¼ëª© í†µí•© í•¨ìˆ˜
def remove_number_if_duplicate(df, col="êµìœ¡ê³¼ì •"):
    df["ë¹„êµìš©"] = df[col].str.replace(r"\d+", "", regex=True)
    dup_list = df["ë¹„êµìš©"].value_counts()
    dup_list = dup_list[dup_list > 1].index.tolist()
    df[col] = df.apply(
        lambda row: re.sub(r"\d+", "", row[col])
        if row["ë¹„êµìš©"] in dup_list
        else row[col],
        axis=1,
    )
    df.drop(columns="ë¹„êµìš©", inplace=True)
    return df


# êµìœ¡ê³¼ì •ë³„ ëŒ€ì¤‘ì†Œ ê³„ì—´ë³„ ë¹„ìœ¨ ê³„ì‚°
@st.cache_data
def get_course_distribution(no_nce):
    """
    êµìœ¡ê³¼ì •ë³„ ëŒ€ì¤‘ì†Œ ê³„ì—´ë³„ ë¹„ìœ¨ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜

    Args:
        no_nce: NCEê°€ ì•„ë‹Œ ë°ì´í„°í”„ë ˆì„

    Returns:
        pd.DataFrame: êµìœ¡ê³¼ì •ë³„ ë¶„ë¥˜ ë¹„ìœ¨ì´ ê³„ì‚°ëœ ë°ì´í„°í”„ë ˆì„
    """
    grouped = (
        no_nce.groupby(["êµìœ¡ê³¼ì •", "ëŒ€ê³„ì—´ë¶„ë¥˜", "ì¤‘ê³„ì—´ë¶„ë¥˜", "ì†Œê³„ì—´ë¶„ë¥˜"])
        .size()
        .reset_index(name="count")
    )
    grouped["ë¹„ìœ¨"] = grouped.groupby("êµìœ¡ê³¼ì •")["count"].transform(
        lambda x: x / x.sum()
    )
    return grouped

@st.cache_data
def load_file_by_extension(file, skiprows=None):
    """
    íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    Args:
        file: ì—…ë¡œë“œëœ íŒŒì¼ ê°ì²´
        skiprows: Excel íŒŒì¼ì˜ ê²½ìš° ê±´ë„ˆë›¸ í–‰ ìˆ˜

    Returns:
        pd.DataFrame: ë¡œë“œëœ ë°ì´í„°í”„ë ˆì„

    Raises:
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì¸ ê²½ìš°
    """
    file_name = file.name.lower()

    if file_name.endswith(".parquet"):
        return pd.read_parquet(file)
    elif file_name.endswith((".xlsx", ".xls")):
        if skiprows is not None:
            return pd.read_excel(file, skiprows=skiprows)
        else:
            return pd.read_excel(file)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_name}")


# === íŒŒì¼ ì—…ë¡œë“œ ===
def upload_file(label, key):
    """
    Streamlit sidebarì—ì„œ íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    Excel, Parquet, CSV íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    return st.sidebar.file_uploader(
        label,
        type=["xlsx", "parquet", "csv"],
        key=key
    )

uploaded_file1 = upload_file("í•™êµë³„ êµìœ¡í¸ì œë‹¨ìœ„ ì •ë³´ íŒŒì¼ (Excel/Parquet/csv)", "file1")
uploaded_file2 = upload_file("êµìœ¡ê³¼ì •_ëŒ€í•™ íŒŒì¼ (Excel/Parquet/csv)", "file2")

# === session_state ì´ˆê¸°í™” ===
if 'data_loaded' not in st.session_state:
    st.session_state.data_loaded = False
if 'results' not in st.session_state:
    st.session_state.results = None

# === ì—…ë¡œë“œ ì™„ë£Œ ë²„íŠ¼ ì¶”ê°€ ===
load_button = st.sidebar.button("ğŸ“¤ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ")

# ë²„íŠ¼ì´ ëˆŒë¦¬ë©´ ë°ì´í„° ë¡œë“œ ì‹œì‘
if load_button and uploaded_file1 and uploaded_file2:
    st.session_state.data_loaded = False  # ì¬ì²˜ë¦¬ ì‹œì‘

# === ë°ì´í„° ì²˜ë¦¬ ë° ê²°ê³¼ í‘œì‹œ ===
if load_button and uploaded_file1 and uploaded_file2:
    if not st.session_state.data_loaded:
        try:
            data_loading_start_time = datetime.now()
            with st.spinner("ğŸ“Š ë°ì´í„°ë¥¼ ë¡œë”©í•˜ëŠ” ì¤‘..."):
                # ë°ì´í„° ë¡œë“œ
                df1 = load_file_by_extension(uploaded_file1, skiprows=4)
                df2 = load_file_by_extension(uploaded_file2, skiprows=8)
                st.success("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")

            # ì´í›„ Step 1 ~ Step 7 ì „ë¶€ ì—¬ê¸°ì— í¬í•¨!
            data_loading_time = datetime.now() - data_loading_start_time
            logger.info(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ ì‹œê°„: {data_loading_time}")

            # ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_bar = st.progress(0, text="ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

            # Step 1: ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
            data_processing_start_time = datetime.now()
            progress_bar.progress(10, text="1/7 ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ ì¤‘...")

            df2 = df2.rename(
                columns={
                    "ì°¨ìˆ˜": "ì¡°ì‚¬ì°¨ìˆ˜",
                    "ë³¸ë¶„êµëª…": "ëŒ€í•™êµ¬ë¶„",
                    "í•™êµêµ¬ë¶„": "ë³¸ë¶„êµ",
                    "í•™ë¶€Â·ê³¼(ì „ê³µ)ì½”ë“œ": "í•™êµë³„í•™ê³¼ì½”ë“œ",
                    "ì£¼ì•¼êµ¬ë¶„ëª…": "ì£¼ì•¼ê°„êµ¬ë¶„",
                    "í•™ë¶€íŠ¹ì„±ëª…": "í•™ê³¼íŠ¹ì„±",
                }
            )

            column_name_change_time = datetime.now() - data_processing_start_time
            logger.info(f"ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½ ì™„ë£Œ ì‹œê°„: {column_name_change_time}")

            # Step 2: ë°ì´í„° ë³‘í•©
            progress_bar.progress(20, text="2/7 ë°ì´í„° ì „ì²˜ë¦¬ ë° ë³‘í•© ì¤‘...")

            data_processing_start_time = datetime.now()
            for df in [df1, df2]:
                df["í•™êµì½”ë“œ"] = df["í•™êµì½”ë“œ"].astype(str).str.strip()
                df["í•™êµë³„í•™ê³¼ì½”ë“œ"] = df["í•™êµë³„í•™ê³¼ì½”ë“œ"].astype(str).str.strip()
                df["í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"] = (
                    df["í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"].astype(str).str.replace(r"\s+", "", regex=True)
                )

            attach_cols = ["êµìœ¡ê³¼ì •", "ì´ìˆ˜êµ¬ë¶„", "í•™ì ", "êµê³¼ëª©í•´ì„¤"]
            df2_slim = df2[
                ["í•™êµì½”ë“œ", "í•™êµë³„í•™ê³¼ì½”ë“œ", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"] + attach_cols
            ].copy()

            merged = pd.merge(
                df1,
                df2_slim,
                on=["í•™êµì½”ë“œ", "í•™êµë³„í•™ê³¼ì½”ë“œ", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"],
                how="left",
                validate="1:m",
            )

            data_processing_time = datetime.now() - data_processing_start_time
            logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„: {data_processing_time}")

            # Step 3: êµìœ¡ê³¼ì • ì •ì œ
            progress_bar.progress(40, text="3/7 êµìœ¡ê³¼ì • ì´ë¦„ ì •ì œ ì¤‘...")
            education_program_correction_start_time = datetime.now()

            # êµìœ¡ê³¼ì • ì •ì œ ì ìš©
            eng2kor = build_eng2kor(merged["êµìœ¡ê³¼ì •"])
            merged["êµìœ¡ê³¼ì •"] = merged["êµìœ¡ê³¼ì •"].apply(
                lambda x: clean_course(x, eng2kor)
            )
            merged = merged.dropna(subset=["êµìœ¡ê³¼ì •"])

            # ìˆ«ì ë²„ì „ ì¤‘ë³µ ì œê±°
            merged = remove_number_if_duplicate(merged, col="êµìœ¡ê³¼ì •")

            merged["í‘œë³¸ìˆ˜ n"] = (
                merged.groupby("êµìœ¡ê³¼ì •")["êµìœ¡ê³¼ì •"].transform("count").astype("Int64")
            )

            education_program_correction_time = (
                datetime.now() - education_program_correction_start_time
            )
            logger.info(
                f"êµìœ¡ê³¼ì • ì´ë¦„ ì •ì œ ì™„ë£Œ ì‹œê°„: {education_program_correction_time}"
            )

            all_code = merged.copy()

            # Step 4: NCE í•„í„°ë§
            progress_bar.progress(50, text="4/7 NCE ë°ì´í„° í•„í„°ë§ ì¤‘...")
            nce_filtering_start_time = datetime.now()
            df_nce = merged[
                (merged["ëŒ€í•™êµ¬ë¶„"] == "ëŒ€í•™")
                & (merged["í•™êµêµ¬ë¶„"] == "ëŒ€í•™êµ")
                & (merged["ë³¸ë¶„êµ"] == "ë³¸êµ")
                & (merged["(ëŒ€í•™)ì§€ì—­"] == "ì„œìš¸")
                & (merged["ì£¼ì•¼ê°„êµ¬ë¶„"] == "ì£¼ê°„")
                & (merged["í•™ê³¼íŠ¹ì„±"] == "ì¼ë°˜ê³¼ì •")
                & (merged["í•™ê³¼ìƒíƒœ"] != "íì§€")
                & (merged["ìˆ˜ì—…ì—°í•œ"] == "4ë…„")
            ].copy()
            nce_filtering_time = datetime.now() - nce_filtering_start_time
            logger.info(f"NCE í•„í„°ë§ ì™„ë£Œ ì‹œê°„: {nce_filtering_time}")

            all_code = all_code.dropna(subset=["êµìœ¡ê³¼ì •"])
            df_nce = df_nce.replace("N.C.E.", "N.C.E", regex=False)
            df_nce = df_nce.drop(columns=["Unnamed: 0"], errors="ignore")

            nce = df_nce[
                (df_nce["ëŒ€ê³„ì—´ë¶„ë¥˜"] == "N.C.E")
                | (df_nce["ì¤‘ê³„ì—´ë¶„ë¥˜"] == "N.C.E")
                | (df_nce["ì†Œê³„ì—´ë¶„ë¥˜"] == "N.C.E")
                | (df_nce["ëŒ€ê³„ì—´ë¶„ë¥˜"] == "ê´‘ì—­ê³„ì—´")
            ]

            no_nce = all_code[
                (all_code["ëŒ€ê³„ì—´ë¶„ë¥˜"] != "N.C.E")
                & (all_code["ì¤‘ê³„ì—´ë¶„ë¥˜"] != "N.C.E")
                & (all_code["ì†Œê³„ì—´ë¶„ë¥˜"] != "N.C.E")
                & (all_code["ì†Œê³„ì—´ë¶„ë¥˜"] != "N.C.E.")
            ]
            no_nce = no_nce[no_nce["êµìœ¡ê³¼ì •"] != "nan"]
            no_nce = no_nce.drop(columns=["Unnamed: 0"], errors="ignore")

            # Step 5: êµìœ¡ê³¼ì •ë³„ ë¹„ìœ¨ ê³„ì‚°
            progress_bar.progress(60, text="5/7 êµìœ¡ê³¼ì •ë³„ ë¶„ë¥˜ ë¹„ìœ¨ ê³„ì‚° ì¤‘...")
            course_distribution_calculation_start_time = datetime.now()
            grouped = get_course_distribution(no_nce)
            course_ratio = grouped.copy()
            course_ratio = course_ratio.replace("N.C.E.", "N.C.E", regex=False)
            course_distribution_calculation_time = (
                datetime.now() - course_distribution_calculation_start_time
            )
            logger.info(
                f"êµìœ¡ê³¼ì •ë³„ ë¶„ë¥˜ ë¹„ìœ¨ ê³„ì‚° ì™„ë£Œ ì‹œê°„: {course_distribution_calculation_time}"
            )

            # Step 6: ì¶”ì²œ ê²°ê³¼ ìƒì„±
            progress_bar.progress(75, text="6/7 ì¶”ì²œ ê²°ê³¼ ìƒì„± ì¤‘...")
            recommendation_result_generation_start_time = datetime.now()
            course_ratio["ì¶”ì²œ_ëŒ€ì¤‘ì†Œ"] = (
                course_ratio["ëŒ€ê³„ì—´ë¶„ë¥˜"].astype(str)
                + "-"
                + course_ratio["ì¤‘ê³„ì—´ë¶„ë¥˜"].astype(str)
                + "-"
                + course_ratio["ì†Œê³„ì—´ë¶„ë¥˜"].astype(str)
            )

            target_courses = nce["êµìœ¡ê³¼ì •"].unique()
            filtered = course_ratio[course_ratio["êµìœ¡ê³¼ì •"].isin(target_courses)].copy()
            filtered = filtered.sort_values(["êµìœ¡ê³¼ì •", "ë¹„ìœ¨"], ascending=[True, False])
            filtered["ì¶”ì²œìˆœìœ„"] = filtered.groupby("êµìœ¡ê³¼ì •").cumcount() + 1

            pivoted = filtered.pivot(
                index="êµìœ¡ê³¼ì •", columns="ì¶”ì²œìˆœìœ„", values=["ì¶”ì²œ_ëŒ€ì¤‘ì†Œ", "ë¹„ìœ¨"]
            ).sort_index(axis=1, level=1)

            new_cols = []
            max_rank = filtered["ì¶”ì²œìˆœìœ„"].max()
            for i in range(1, max_rank + 1):
                new_cols.append(("ì¶”ì²œ_ëŒ€ì¤‘ì†Œ", i))
                new_cols.append(("ë¹„ìœ¨", i))
            pivoted = pivoted[new_cols]

            pivoted.columns = [
                f"ëŒ€ì¤‘ì†Œ_{col[1]}ìˆœìœ„{'_í™•ë¥ ' if col[0] == 'ë¹„ìœ¨' else ''}"
                for col in pivoted.columns
            ]
            pivoted = pivoted.reset_index()

            merged_result = df_nce.merge(pivoted, how="left")

            recommendation_result_generation_time = (
                datetime.now() - recommendation_result_generation_start_time
            )
            logger.info(
                f"ì¶”ì²œ ê²°ê³¼ ìƒì„± ì™„ë£Œ ì‹œê°„: {recommendation_result_generation_time}"
            )

            # Step 7: ì „ê³µë³„ ì¶”ì²œ ìƒì„±
            progress_bar.progress(90, text="7/7 ì „ê³µë³„ ì¶”ì²œ ê²°ê³¼ ìƒì„± ì¤‘...")
            major_recommendation_generation_start_time = datetime.now()
            nce_keys = nce[["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"]].drop_duplicates()
            all_code_final = merged_result.merge(
                nce_keys, on=["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"], how="inner"
            )

            all_code_final["í‘œë³¸ìˆ˜ n"] = pd.to_numeric(
                all_code_final["í‘œë³¸ìˆ˜ n"], errors="coerce"
            ).fillna(1)

            rank_cols = [
                c
                for c in all_code_final.columns
                if c.startswith("ëŒ€ì¤‘ì†Œ_") and not c.endswith("_í™•ë¥ ")
            ]
            prob_cols = [
                c
                for c in all_code_final.columns
                if c.startswith("ëŒ€ì¤‘ì†Œ_") and c.endswith("_í™•ë¥ ")
            ]

            long_list = []
            for i in range(1, len(rank_cols) + 1):
                cat_col = f"ëŒ€ì¤‘ì†Œ_{i}ìˆœìœ„"
                prob_col = f"ëŒ€ì¤‘ì†Œ_{i}ìˆœìœ„_í™•ë¥ "
                if cat_col in all_code_final.columns and prob_col in all_code_final.columns:
                    sub = all_code_final[
                        [
                            "í•™êµëª…",
                            "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…",
                            "êµìœ¡ê³¼ì •",
                            "í‘œë³¸ìˆ˜ n",
                            cat_col,
                            prob_col,
                        ]
                    ].copy()
                    sub.columns = [
                        "í•™êµëª…",
                        "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…",
                        "êµìœ¡ê³¼ì •",
                        "í‘œë³¸ìˆ˜",
                        "ëŒ€ì¤‘ì†Œ",
                        "í™•ë¥ ",
                    ]
                    long_list.append(sub)

            melted = pd.concat(long_list, ignore_index=True).dropna(
                subset=["ëŒ€ì¤‘ì†Œ", "í™•ë¥ "]
            )

            # ë²¡í„°í™”ëœ ì—°ì‚°ìœ¼ë¡œ ìµœì í™” (apply ëŒ€ì‹ )
            melted['ê°€ì¤‘ê°’'] = melted['í™•ë¥ '] * melted['í‘œë³¸ìˆ˜']

            grouped = melted.groupby(
                ["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…", "êµìœ¡ê³¼ì •", "ëŒ€ì¤‘ì†Œ"]
            ).agg({
                'ê°€ì¤‘ê°’': 'sum',
                'í‘œë³¸ìˆ˜': 'sum'
            }).reset_index()

            grouped['ê°€ì¤‘í™•ë¥ '] = grouped['ê°€ì¤‘ê°’'] / grouped['í‘œë³¸ìˆ˜']
            agg = grouped[["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…", "êµìœ¡ê³¼ì •", "ëŒ€ì¤‘ì†Œ", "ê°€ì¤‘í™•ë¥ "]].copy()

            summed = agg.groupby(["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…", "ëŒ€ì¤‘ì†Œ"], as_index=False)[
                "ê°€ì¤‘í™•ë¥ "
            ].sum()

            summed["ì •ê·œí™”í™•ë¥ "] = summed.groupby(["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"])[
                "ê°€ì¤‘í™•ë¥ "
            ].transform(lambda x: x / x.sum())

            summed["ìˆœìœ„"] = (
                summed.groupby(["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"])["ì •ê·œí™”í™•ë¥ "]
                .rank(method="first", ascending=False)
                .astype(int)
            )

            pivoted_final = summed.pivot(
                index=["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"],
                columns="ìˆœìœ„",
                values=["ëŒ€ì¤‘ì†Œ", "ì •ê·œí™”í™•ë¥ "],
            ).sort_index(axis=1, level=1)

            new_cols = []
            max_rank = summed["ìˆœìœ„"].max()
            for i in range(1, max_rank + 1):
                new_cols.append(("ëŒ€ì¤‘ì†Œ", i))
                new_cols.append(("ì •ê·œí™”í™•ë¥ ", i))
            pivoted_final = pivoted_final[new_cols]

            pivoted_final.columns = [
                f"ì¶”ì²œ_ëŒ€ì¤‘ì†Œ_{col[1]}ìˆœìœ„"
                if col[0] == "ëŒ€ì¤‘ì†Œ"
                else f"ì¶”ì²œ_í™•ë¥ _{col[1]}ìˆœìœ„"
                for col in pivoted_final.columns
            ]
            pivoted_final = pivoted_final.reset_index()

            course_ratio_result_nce = pivoted_final.copy()

            major_recommendation_generation_time = (
                datetime.now() - major_recommendation_generation_start_time
            )
            logger.info(
                f"ì „ê³µë³„ ì¶”ì²œ ê²°ê³¼ ìƒì„± ì™„ë£Œ ì‹œê°„: {major_recommendation_generation_time}"
            )
            logger.info(f"ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ì‹œê°„: {datetime.now() - data_loading_start_time}")

            progress_bar.progress(100, text="âœ… ì²˜ë¦¬ ì™„ë£Œ!")

            # ê²°ê³¼ë¥¼ session_stateì— ì €ì¥
            st.session_state.results = {
                'course_ratio_result_nce': course_ratio_result_nce,
                'merged_result': merged_result,
                'nce_keys': nce_keys,
                'nce': nce,
                'max_rank': max_rank
            }
            st.session_state.data_loaded = True

            # ì„±ê³µ ë©”ì‹œì§€
            st.success("âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.exception(e)
            st.session_state.data_loaded = False
            st.stop()  # ì—ëŸ¬ ë°œìƒ ì‹œ ì—¬ê¸°ì„œ ë©ˆì¶¤

# ê²°ê³¼ í‘œì‹œ (ë°ì´í„°ê°€ ë¡œë“œëœ ê²½ìš°)
if st.session_state.data_loaded and st.session_state.results:
    results = st.session_state.results
    course_ratio_result_nce = results['course_ratio_result_nce']
    merged_result = results['merged_result']
    nce_keys = results['nce_keys']
    nce = results['nce']
    max_rank = results['max_rank']

    # === ê²°ê³¼ í‘œì‹œ ===
    st.markdown("---")
    st.header("ğŸ“Š ë¶„ì„ ê²°ê³¼")

    # íƒ­ìœ¼ë¡œ êµ¬ë¶„
    tab1, tab2, tab3 = st.tabs(
        ["ğŸ“ˆ í†µê³„ ì •ë³´", "ğŸ¯ ì „ê³µë³„ ì¶”ì²œ ê²°ê³¼", "ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"]
    )

    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì „ì²´ ì „ê³µ ìˆ˜", f"{len(nce_keys):,}ê°œ")
        with col2:
            st.metric("NCE ê³¼ëª© ìˆ˜", f"{len(nce):,}ê°œ")
        with col3:
            st.metric("ì¶”ì²œ ê²°ê³¼ ìƒì„±", f"{len(course_ratio_result_nce):,}ê±´")
        with col4:
            st.metric("ìµœëŒ€ ì¶”ì²œ ìˆœìœ„", f"{max_rank}ìˆœìœ„")

        st.markdown("### ëŒ€ê³„ì—´ ë¶„í¬")
        major_dist = nce["ëŒ€ê³„ì—´ë¶„ë¥˜"].value_counts()
        st.bar_chart(major_dist)

    with tab2:
        st.markdown("### ì „ê³µë³„ í‘œì¤€ë¶„ë¥˜ì²´ê³„ ì¶”ì²œ ê²°ê³¼")

        # ìƒìœ„ 5ê°œ ìˆœìœ„ë§Œ í‘œì‹œ (ê³ ì •)
        num_ranks = 5
        display_cols = ["í•™êµëª…", "í•™ë¶€Â·ê³¼(ì „ê³µ)ëª…"]
        for i in range(1, num_ranks + 1):
            display_cols.append(f"ì¶”ì²œ_ëŒ€ì¤‘ì†Œ_{i}ìˆœìœ„")
            display_cols.append(f"ì¶”ì²œ_í™•ë¥ _{i}ìˆœìœ„")

        display_cols = [col for col in display_cols if col in course_ratio_result_nce.columns]

        st.dataframe(course_ratio_result_nce[display_cols], use_container_width=True, height=500)

        st.info(f"ğŸ“Œ ì´ {len(course_ratio_result_nce)}ê°œ ì „ê³µì˜ ì¶”ì²œ ê²°ê³¼ (ìƒìœ„ 5ìˆœìœ„ í‘œì‹œ)")

    with tab3:
        st.markdown("### ê²°ê³¼ ë‹¤ìš´ë¡œë“œ")

        # Excel ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
        def to_excel(df):
            output = BytesIO()
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                df.to_excel(writer, index=False, sheet_name="ì¶”ì²œê²°ê³¼")
            return output.getvalue()

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ì „ê³µë³„ ì¶”ì²œ ê²°ê³¼")
            excel_data1 = to_excel(course_ratio_result_nce)
            st.download_button(
                label="ğŸ“¥ ì „ê³µë³„ Excel ë‹¤ìš´ë¡œë“œ",
                data=excel_data1,
                file_name="nce_ì „ê³µë³„_ì¶”ì²œê²°ê³¼.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
            st.caption(f"ğŸ“Š {len(course_ratio_result_nce):,}ê°œ ì „ê³µ")

        with col2:
            st.markdown("#### êµìœ¡ê³¼ì •ë³„ ì¶”ì²œ ê²°ê³¼")
            excel_data2 = to_excel(merged_result)
            st.download_button(
                label="ğŸ“¥ êµìœ¡ê³¼ì •ë³„ Excel ë‹¤ìš´ë¡œë“œ",
                data=excel_data2,
                file_name="nce_êµìœ¡ê³¼ì •ë³„_ì¶”ì²œê²°ê³¼.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            )
            st.caption(f"ğŸ“Š {len(merged_result):,}ê°œ êµìœ¡ê³¼ì •")

elif not st.session_state.data_loaded and uploaded_file1 and uploaded_file2:
    st.info("ğŸ“¥ ë‘ íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„, **'ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ' ë²„íŠ¼**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ í•„ìš”í•œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

    st.markdown("""
    ### ğŸ“‹ ì‚¬ìš© ë°©ë²•

    1. **íŒŒì¼ ì¤€ë¹„**
       - `í•™êµë³„ êµìœ¡í¸ì œë‹¨ìœ„ ì •ë³´_YYYYMMDDê¸°ì¤€.xlsx`
       - `êµìœ¡ê³¼ì •_ëŒ€í•™(YYYYMMDD).xlsx`

    2. **íŒŒì¼ ì—…ë¡œë“œ**
       - ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ ë‘ ê°œì˜ Excel íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.

    3. **ê²°ê³¼ í™•ì¸**
       - 'ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ' í´ë¦­ ì‹œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
       - í•™êµë³„, ì „ê³µë³„ í•„í„°ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

    4. **ê²°ê³¼ ë‹¤ìš´ë¡œë“œ**
       - ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ### ğŸ“Š ì£¼ìš” ê¸°ëŠ¥

    - âœ… NCE(ë¶„ë¥˜ ë¶ˆê°€) ì „ê³µì— ëŒ€í•œ í‘œì¤€ë¶„ë¥˜ì²´ê³„ ìë™ ì¶”ì²œ
    - âœ… êµìœ¡ê³¼ì • ê¸°ë°˜ í™•ë¥  ê³„ì‚°
    - âœ… ì „ê³µë³„ ìˆœìœ„ë³„ ì¶”ì²œ ê²°ê³¼ ì œê³µ
    - âœ… í•™êµë³„ í•„í„°ë§ ë° ì‹œê°í™”
    - âœ… Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    """)

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "NCE ì „ê³µ í‘œì¤€ë¶„ë¥˜ì²´ê³„ ì¶”ì²œ ì‹œìŠ¤í…œ v1.0"
    "</div>",
    unsafe_allow_html=True,
)